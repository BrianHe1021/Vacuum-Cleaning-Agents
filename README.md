# Vacuum-Cleaning-Agents
The vacuum cleaner environment consists of 10 X 10 grid, which needs to be cleaned. There are 3 percepts: a wall sensor = 1 if the machine has a wall right in the front and 0 otherwise, a dirt sensor = 1 if the square contains dirt, and a home sensor = 1 if the agent is home (the starting location). Five actions are available: go forward, turn right by 90 degrees, turn left by 90 degrees, suck up dirt, and turn off. The agent always starts in the bottom leftmost corner oriented upwards. It is evaluated by the total number of clean cells as a function of the number of actions taken. (Plot the number of clean cells vs. the number of actions taken). Implement three agent programs as listed as follows. In each case, the agent program should be organized as a list of if-then rules. The first rule whose condition evaluates to TRUE will fire, which means the corresponding action is executed. If the agent program is deterministic then there is only one possible action selected at each step. If it is stochastic, then there are several actions, and each action will be chosen with some probability. For example, the rule "if wall then turn-left 0.5 turn-right 0.5" will choose turn-left and turn-right with equal probability if the wall sensor is ON.

A simple memory-less deterministic reflex agent.
A randomized reflex agent that can choose actions randomly based on sensor readings.
A deterministic model-based reflex agent with a small amount (2 to 3 bits) of memory that represents its "state." When executing each action, the agent simultaneously updates the state by setting or resetting these bits according to how you specify them. The actions can be based on its current state bits as well as the current percepts.

There are two different environments. In the first environment, the grid is empty. In the second, divide the grid into 4 rooms with doors (one grid cell) in between every pair of adjacent rooms. In each case, for the random agent, repeat the experiment 50 times and plot the average performance over the 50 runs. For other agents, one run for each environment would do. Note that your agents may not always succeed in cleaning both environments.
